{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d4caaf-589d-42d4-98ce-c60d22114206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2489d53-0d2a-4bd4-aae4-4466c5994426",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2a48431-3121-4312-8a76-770af33e7d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "# Download and load Shakespeare dataset\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "# Read the text file\n",
    "with open(path_to_file, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Check the first 500 characters of the text\n",
    "print(text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14244585-b3c6-416d-b18c-1cbea66a7249",
   "metadata": {},
   "source": [
    "# Preprocess Text\n",
    "Convert the text into a numeric representation and prepare sequences of input and target text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb151a28-ec11-46a8-930f-0585c391be12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n",
      "Text as integers: [18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56]\n"
     ]
    }
   ],
   "source": [
    "# Unique characters in the text\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')\n",
    "\n",
    "# Creating a mapping from unique characters to indices\n",
    "char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
    "index_to_char = np.array(vocab)\n",
    "\n",
    "# Convert text to integers\n",
    "text_as_int = np.array([char_to_index[c] for c in text])\n",
    "\n",
    "print(f'Text as integers: {text_as_int[:20]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be3c5e3f-5a93-4159-9b1b-6b3b68b288db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# Define sequence length and create training examples\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "\n",
    "# Create the dataset by slicing text\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# Create input-target sequences\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "# Function to split input and target\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "# Display a sample input and target sequence\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(f'Input: {repr(\"\".join([index_to_char[i] for i in input_example.numpy()]))}')\n",
    "    print(f'Target: {repr(\"\".join([index_to_char[i] for i in target_example.numpy()]))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef53e5a-5003-42a3-831c-d6410e410607",
   "metadata": {},
   "source": [
    "# Prepare Training Dataset\n",
    "Batch the dataset and shuffle it for better learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315d4462-6010-4fbd-b72c-a5579becd4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# Create the batches\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a549124-97d6-4af2-915a-921fac34f916",
   "metadata": {},
   "source": [
    "# Build the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10db9601-e433-4566-a3a5-66b97c57d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Length of the vocabulary in chars\n",
    "embedding_dim = 256  # Dimension of the embedding layer\n",
    "rnn_units = 1024     # Number of LSTM units\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_model(vocab_size=len(vocab), embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f536c62-f768-40e7-a68e-6693843cc639",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e0ec81-ed4e-4e0a-821e-387797d626d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a52fd43-fa40-43b9-b94a-09c65d8b151e",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc4e255e-0311-4789-a630-5dadf23628f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "172/172 [==============================] - 520s 3s/step - loss: 2.6055\n",
      "Epoch 2/30\n",
      "172/172 [==============================] - 520s 3s/step - loss: 1.8947\n",
      "Epoch 3/30\n",
      "172/172 [==============================] - 523s 3s/step - loss: 1.6409\n",
      "Epoch 4/30\n",
      "172/172 [==============================] - 521s 3s/step - loss: 1.5063\n",
      "Epoch 5/30\n",
      "172/172 [==============================] - 513s 3s/step - loss: 1.4247\n",
      "Epoch 6/30\n",
      "172/172 [==============================] - 494s 3s/step - loss: 1.3682\n",
      "Epoch 7/30\n",
      "172/172 [==============================] - 501s 3s/step - loss: 1.3232\n",
      "Epoch 8/30\n",
      "172/172 [==============================] - 500s 3s/step - loss: 1.2858\n",
      "Epoch 9/30\n",
      "172/172 [==============================] - 490s 3s/step - loss: 1.2497\n",
      "Epoch 10/30\n",
      "172/172 [==============================] - 495s 3s/step - loss: 1.2160\n",
      "Epoch 11/30\n",
      "172/172 [==============================] - 492s 3s/step - loss: 1.1825\n",
      "Epoch 12/30\n",
      "172/172 [==============================] - 496s 3s/step - loss: 1.1471\n",
      "Epoch 13/30\n",
      "172/172 [==============================] - 500s 3s/step - loss: 1.1115\n",
      "Epoch 14/30\n",
      "172/172 [==============================] - 496s 3s/step - loss: 1.0733\n",
      "Epoch 15/30\n",
      "172/172 [==============================] - 492s 3s/step - loss: 1.0355\n",
      "Epoch 16/30\n",
      "172/172 [==============================] - 485s 3s/step - loss: 0.9968\n",
      "Epoch 17/30\n",
      "172/172 [==============================] - 484s 3s/step - loss: 0.9566\n",
      "Epoch 18/30\n",
      "172/172 [==============================] - 485s 3s/step - loss: 0.9164\n",
      "Epoch 19/30\n",
      "172/172 [==============================] - 489s 3s/step - loss: 0.8782\n",
      "Epoch 20/30\n",
      "172/172 [==============================] - 488s 3s/step - loss: 0.8398\n",
      "Epoch 21/30\n",
      "172/172 [==============================] - 500s 3s/step - loss: 0.8047\n",
      "Epoch 22/30\n",
      "172/172 [==============================] - 502s 3s/step - loss: 0.7689\n",
      "Epoch 23/30\n",
      "172/172 [==============================] - 492s 3s/step - loss: 0.7384\n",
      "Epoch 24/30\n",
      "172/172 [==============================] - 491s 3s/step - loss: 0.7086\n",
      "Epoch 25/30\n",
      "172/172 [==============================] - 488s 3s/step - loss: 0.6806\n",
      "Epoch 26/30\n",
      "172/172 [==============================] - 493s 3s/step - loss: 0.6568\n",
      "Epoch 27/30\n",
      "172/172 [==============================] - 490s 3s/step - loss: 0.6343\n",
      "Epoch 28/30\n",
      "172/172 [==============================] - 494s 3s/step - loss: 0.6129\n",
      "Epoch 29/30\n",
      "172/172 [==============================] - 503s 3s/step - loss: 0.5941\n",
      "Epoch 30/30\n",
      "172/172 [==============================] - 511s 3s/step - loss: 0.5774\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs for training\n",
    "EPOCHS = 30  # Increase this for better results\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d1ea8a-6b53-49f1-aaaf-808073cc20c5",
   "metadata": {},
   "source": [
    "# Rebuild the Model for Inference\n",
    "Rebuild the model with a batch size of 1 for text generation and load the trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d844b26-5865-47aa-b22a-9e4373cba56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the necessary variables\n",
    "vocab_size = len(vocab)  # Recalculate vocab_size in case it was lost\n",
    "\n",
    "# Rebuild the model with batch size 1 for inference\n",
    "model_for_inference = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# Load the trained weights into the new model\n",
    "model_for_inference.set_weights(model.get_weights())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7c76e-88f0-4b7b-a1f6-711a59d7f2a4",
   "metadata": {},
   "source": [
    "# Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5689566-8f76-4de5-b1b9-28569f2c71f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "    \n",
    "    # Convert start string to numbers (vectorize)\n",
    "    input_eval = [char_to_index[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    # Empty string to store results\n",
    "    text_generated = []\n",
    "    \n",
    "    # Temperature affects the creativity of the generated text\n",
    "    temperature = 0.5  # Lower for more predictable text, higher for more diverse text\n",
    "    \n",
    "    # Reset model states\n",
    "    model.reset_states()\n",
    "    \n",
    "    for _ in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        # Sample from the prediction distribution\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "        \n",
    "        # Add the predicted character to the input\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(index_to_char[predicted_id])\n",
    "    \n",
    "    return start_string + ''.join(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f223a6f-4dd8-4bef-becc-62c7013ceb5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: I fain about your highness to my wife.\n",
      "\n",
      "BAPTISTA:\n",
      "Now, good my lord, let me hear him.\n",
      "\n",
      "Shepherd:\n",
      "None, sir; I know you fare you well, for I must wed;\n",
      "But I will bring thee tidings, Call himself,\n",
      "And spit it bleeding in thy country's breast,\n",
      "And cannot live but to the senate-house.\n",
      "\n",
      "KING EDWARD IV:\n",
      "I would they would all for our state was banish'd:\n",
      "And if I be not, hear'st me from the world,\n",
      "To seek their fury on'd all in vain;\n",
      "The sorrow that I was wont to come home.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Faith, we may longer be a hundred throne.\n",
      "\n",
      "GLOUCESTER:\n",
      "So many give me leave to look on him.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "The northest issue like a treeban's face.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "But how like you be well; where is he?\n",
      "\n",
      "CATESBY:\n",
      "Rescue, my Lord of Somerset, what would\n",
      "Lear holy friar, that they have forget me\n",
      "To banish them when I drink to thee again.\n",
      "Therefore they thought it good or more your brother's love.\n",
      "\n",
      "First Citizen:\n",
      "We are all undone, we must be gone. We are not fit\n",
      "with an aspect more than any other of him.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate text using the updated model\n",
    "print(generate_text(model_for_inference, start_string=\"ROMEO: \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
